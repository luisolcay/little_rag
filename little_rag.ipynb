{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ab0058",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "\n",
    "#### Step 0: Set environment\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d839f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f107a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGCHAIN_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "FOLDER_PATH = os.getenv('FOLDER_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32b764",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "\n",
    "#### Step 1: Split text, create/embed chunks and load chunks\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfe5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olcay\\LuisOlcay20\\python\\IA\\RAG\\rag_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488f1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 documents loaded\n",
      "1261 chunks in total\n"
     ]
    }
   ],
   "source": [
    "#variable to split text\n",
    "from numpy.core.defchararray import endswith\n",
    "\n",
    "\n",
    "split_text = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "#function to load_documents\n",
    "def load_documents(FOLDER_PATH):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(FOLDER_PATH):\n",
    "        file_path = os.path.join(FOLDER_PATH,file_name)\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_name.endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"The document {file_name} is not supported\")\n",
    "        \n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "#load documents\n",
    "documents = load_documents(FOLDER_PATH)\n",
    "print(f\"{len(documents)} documents loaded\")\n",
    "\n",
    "#split text of the documents\n",
    "chunks = split_text.split_documents(documents)\n",
    "print(f\"{len(chunks)} chunks in total\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e63b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call embedding model from openai\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82302595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_db = Chroma().from_documents(collection_name='collahuasi_pdfs',documents=chunks,embedding=embeddings, persist_directory='./cllh_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a05473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create retriever\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8ef4a",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "#### Step 2: Start to create the chain\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e690bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call to model\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028cdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple parse answer of the model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser =  StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6e6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "You are an expert in environmental consulting projects in the north of Chile. \n",
    "Always answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f3be3",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "#### ⚙️ Step-by-step flow of the `rag_chain`\n",
    "\n",
    "1. **Input**  \n",
    "   `\"tell me the height of the Collahuasi campament\"`\n",
    "\n",
    "2. **Branch mapping**  \n",
    "   - **context** → input goes to the `retriever`, which returns `docs`.  \n",
    "     The lambda joins all document texts into one string using `\"\\n\\n\"`.  \n",
    "   - **question** → `RunnablePassthrough()` passes the original input unchanged.\n",
    "\n",
    "3. **Prompt**  \n",
    "   The `prompt` fills its template with `{context}` and `{question}`.\n",
    "\n",
    "4. **LLM**  \n",
    "   The `llm` generates an answer based on the formatted prompt.\n",
    "\n",
    "5. **Parser**  \n",
    "   The `parser` formats or extracts the model’s output (e.g., plain text or JSON).\n",
    "\n",
    "**Result:**  \n",
    "A final, parsed answer based on the retrieved context and user question.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3035b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# retrievers output  its docs = []\n",
    "rag_chain = (\n",
    "    {\n",
    "            \"context\":retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])), \n",
    "            \"question\": RunnablePassthrough() } \n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser \n",
    ")\n",
    "\n",
    "#rag_chain.invoke('tell me the height of the collahuasi campament')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058581b",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "#### Step 3 ⚙️ Add history-aware to the chat\n",
    "\n",
    "1. **Input**  \n",
    "   User asks a question.\n",
    "\n",
    "2. **History retriever**  \n",
    "   Reformulates the question using `chat_history`, queries the retriever, and returns relevant docs.\n",
    "\n",
    "3. **Context branch**  \n",
    "   Joins all `page_content` from docs with `\"\\n\\n\"` → becomes `{context}`.\n",
    "\n",
    "4. **Prompt**  \n",
    "   Combines `{chat_history}`, `{context}`, and `{question}` into the `answer_prompt`.\n",
    "\n",
    "\n",
    "**Result:**  \n",
    "A context-aware answer built from retrieved documents and chat history.\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a768ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Base Message\n",
    "chat_history: list = []\n",
    "\n",
    "question = 'tell me the height of the collahuasi campament'\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=answer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c8bceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "contextualize_history_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given the chat history and the latest user question, rewrite it self-contained.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "history_retriever = create_history_aware_retriever(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    prompt=contextualize_history_prompt\n",
    ")\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use the provided context to answer the question. \"\n",
    "    \"If the answer is not present, say you don't know.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"system\", \"Context:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "context_branch = (\n",
    "    {\n",
    "        \"input\": RunnablePassthrough(),          \n",
    "        \"chat_history\": lambda _: chat_history    \n",
    "    }\n",
    "    | history_retriever                           #history_retriever use get.\n",
    "    | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf1dbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":  context_branch,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": lambda _: chat_history,  # se inyecta al prompt\n",
    "    }\n",
    "    | answer_prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af105f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context refers to the Collahuasi mining project, which is located in the altiplano of the Atacama Desert.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('of which project are we talking about')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
